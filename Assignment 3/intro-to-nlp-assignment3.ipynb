{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Assignment 3\n",
    "\n",
    "## Assignment on Unsupervised Learning in NLP\n",
    "\n",
    "### Description of Assignment 3\n",
    "\n",
    "This assignment relates to Theme 3 of the Introduction to NLP course and will focus on topics of unsupervised learning in NLP.\n",
    "The data set to be used in the assignment is abstracts,zip file. Use  the files in the subdirectory \"awards_2002\" as the corpus in the exercises. Follow the code examples in the lecture notebook and adapt them to work with this data, and perform the following steps described in the assignment steps/Questions section.\n",
    "\n",
    "**Assignment steps/Questions:**\n",
    "\n",
    "1. **Topic Analysis**\n",
    "   * There are quantitative measures for evaluating various aspects of clustering results and topic models intrinsically, and they can also be evaluated extrinsically by how well the clusters/topics serve some supervised task as features. In this exercise, however, we will focus on qualitative evaluation of the results in terms of their descriptiveness. As in the example code, you may limit yourself to the 1000 first documents of the corpus when performing clustering, in order to simplify the task and speed up experimentation, but use the whole corpus to calculate tf-idf features.\n",
    "   * **a.** Experiment with different setups of the tf-idf feature extraction and clustering (k-means or hierarchical), in order to obtain meaningful results. When you arrive at a good configuration, describe it and motivate your chosen setup/parameters.\n",
    "   * **b.** Inspect the keywords of the clusters. List the 10 first clusters out of all (i.e., not cherry picked examples) and provide an as descriptive label as possible for each of them. \n",
    "   * **c.** Select one or two good clusters (that can be clearly interpreted) and one or two bad clusters (that might be difficult to interpret or distinguish). Motivate your choise (clusters may, for instance, be overlapping, too broad/narrow or incoherent). \n",
    "   * **d.** Repeat the experiment in (a) with LDA topic modeling instead (on the whole corpus), and explain briefly how the results compare to your previously chosen clustering setup. A few concrete examples may be helpful. Do your best to make sure the list of topic keywords are informative through appropriate post-processing.\n",
    "\n",
    "2. **Word Vectors**\n",
    "   * **a.** Choose about 5 words (arbitrarily) to use as seed words in the following experiment. Train word2vec vectors on the corpus while trying out variations on the parameters. Evaluate the vector models by inspecting the most similar words for each of the seed words, and try to identify qualitative differences between different parameter choices. Which parameters seem to have the most interesting effect? At what values? Motivate. Finally study the qualitative effect of increasing the training data, by similarly comparing vectors trained with the best setup on the texts from the awards_2002 directory against vectors trained on the whole set of abstracts (1990-2002).\n",
    "   * **b.** Repeat the experiment with ELMo from the lecture, with a different target word and different sentences. Choose a word that can have multiple senses, and construct 10 sentences that express 2-3 different senses of the word. Produce ELMo embeddins for the target word in each sentence and measure the similarity between the vectors. Evaluate in how many cases the measured similarities can be used to successfully distinguish between the different senses. Comment on the results, e.g., are you able to identify a particular way in which the model fails?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import heapq, numpy as np\n",
    "#!pip3 install gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "\n",
    "for subdirectory in os.listdir(os.getcwd() + '/awards_2002'):\n",
    "    for filename in  os.listdir(os.getcwd() + '/awards_2002/' + subdirectory):\n",
    "        path = os.getcwd() + '/awards_2002/' + subdirectory + \"/\" + filename\n",
    "        with open(path, 'rt', encoding='latin1')as f:\n",
    "            readNextLines = False\n",
    "            abstractStr = \"\"\n",
    "            for line in f.readlines():\n",
    "                if(readNextLines):\n",
    "                    abstractStr += line.strip() + \" \"\n",
    "                if(line.startswith('Abstract    :')):\n",
    "                    readNextLines = True\n",
    "            abstracts.append(abstractStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Topic Analysis\n",
    "**There are quantitative measures for evaluating various aspects of clustering results and topic models intrinsically, and they can also be evaluated extrinsically by how well the clusters/topics serve some supervised task as features. In this exercise, however, we will focus on qualitative evaluation of the results in terms of their descriptiveness. As in the example code, you may limit yourself to the 1000 first documents of the corpus when performing clustering, in order to simplify the task and speed up experimentation, but use the whole corpus to calculate tf-idf features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a - Experiment with different setups of the tf-idf feature extraction and clustering (k-means or hierarchical), in order to obtain meaningful results. When you arrive at a good configuration, describe it and motivate your chosen setup/parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 20000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True, sublinear_tf=True, max_df=1.0, max_features=20000, ngram_range=(1,1))\n",
    "# Tip: the vectorizer also supports extracting n-gram features (common short sequences of words), which may be more descriptive but also much less frequent\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9637\tthe\n",
      "9619\tof\n",
      "9613\tand\n",
      "9511\tto\n",
      "9442\tin\n",
      "8743\tthis\n",
      "8625\tfor\n",
      "8269\tis\n",
      "8228\twill\n",
      "7632\tbe\n",
      "7419\ton\n",
      "7271\twith\n",
      "7167\tthat\n",
      "6656\tare\n",
      "6642\tresearch\n",
      "6561\tby\n",
      "6424\tas\n",
      "5968\tfrom\n",
      "5750\tan\n",
      "5402\tthese\n"
     ]
    }
   ],
   "source": [
    "terms_in_docs = tfidf_vectorizer.inverse_transform(tfidf_matrix)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.29\tchow\n",
      "0.19\thodge\n",
      "0.19\talgebraic\n",
      "0.14\tgeometry\n",
      "0.14\tsubgroup\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.22\tcultivating\n",
      "0.21\tethnically\n",
      "0.20\tcontrol\n",
      "0.20\texchanging\n",
      "0.18\tamerican\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.30\tupdating\n",
      "0.20\treference\n",
      "0.18\tsecondly\n",
      "0.17\tmethod\n",
      "0.14\tsimulation\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.23\tconference\n",
      "0.20\tcomputations\n",
      "0.19\tlearn\n",
      "0.18\ttheory\n",
      "0.18\tgroup\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.21\tuncontrollable\n",
      "0.20\tcommonality\n",
      "0.19\tuncertainties\n",
      "0.18\tpreferences\n",
      "0.18\talternative\n"
     ]
    }
   ],
   "source": [
    "## Inspect top terms per document\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features,tfidf_matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sample = tfidf_matrix[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=30, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=123, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Do clustering\n",
    "km = KMeans(n_clusters=30, random_state=123, verbose=0)\n",
    "km.fit(matrix_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (4 docs)\n",
      "proofs, resultants, theorems, nova, lemma, footage, analogy, pbs, logarithmic, wgbh\n",
      "\n",
      "Cluster: 1 (88 docs)\n",
      "equations, dynamical, fluid, nonlinear, random, waves, solutions, fluids, mesoscale, schrodinger\n",
      "\n",
      "Cluster: 2 (46 docs)\n",
      "colleges, alliance, technicians, college, umeb, manufacturing, curriculum, workforce, technical, industry\n",
      "\n",
      "Cluster: 3 (43 docs)\n",
      "available, not, zygotic, zygomycota, zygomycetes, zygmund, zurich, zuni, zro2, zr\n",
      "\n",
      "Cluster: 4 (56 docs)\n",
      "conference, algebraic, contract, arsenic, seminar, meeting, stochastic, statistical, grid, haiwee\n",
      "\n",
      "Cluster: 5 (13 docs)\n",
      "oceanographic, vessel, fleet, ship, ships, shipboard, operated, ctd, unols, equipment\n",
      "\n",
      "Cluster: 6 (26 docs)\n",
      "fellowship, mathematical, sciences, fellowships, zygotic, zygomycota, zygomycetes, zygmund, zurich, zuni\n",
      "\n",
      "Cluster: 7 (30 docs)\n",
      "algebras, representation, lie, symmetries, theory, representations, finite, sheaves, quantum, automorphic\n",
      "\n",
      "Cluster: 8 (45 docs)\n",
      "workshop, cmes, costa, cme, sediment, solar, coronagraph, tectonically, subduction, arc\n",
      "\n",
      "Cluster: 9 (20 docs)\n",
      "microbial, fellowship, biology, bacterial, fungal, training, fungi, bacteria, host, virulence\n",
      "\n",
      "Cluster: 10 (19 docs)\n",
      "quantum, straightening, networks, required, no, neural, repair, abstract, algorithm, heat\n",
      "\n",
      "Cluster: 11 (44 docs)\n",
      "nigms, protein, panel, biology, rna, genes, transcription, packages, nucleic, duplication\n",
      "\n",
      "Cluster: 12 (49 docs)\n",
      "manifolds, spaces, curvature, conformal, geometry, equations, geometric, operators, operator, einstein\n",
      "\n",
      "Cluster: 13 (41 docs)\n",
      "teachers, teacher, mathematics, college, classroom, assistive, school, middle, classrooms, competencies\n",
      "\n",
      "Cluster: 14 (16 docs)\n",
      "fellowship, postdoctoral, he, informatics, redshift, galaxies, training, she, stars, astronomy\n",
      "\n",
      "Cluster: 15 (44 docs)\n",
      "optical, semiconductor, ultrabroadband, fiber, wavelength, plasma, ionosphere, dots, cavity, surfaces\n",
      "\n",
      "Cluster: 16 (10 docs)\n",
      "crcd, curriculum, entitled, notre, dame, communications, photovoltaic, pittsburgh, ethics, biomolecular\n",
      "\n",
      "Cluster: 17 (13 docs)\n",
      "czech, crystals, films, crystal, doped, cytokinin, homotopy, telluride, transparent, inorganic\n",
      "\n",
      "Cluster: 18 (61 docs)\n",
      "terrorist, tropical, attitudes, attacks, archaeological, instrument, harvard, metropolitan, russian, pacific\n",
      "\n",
      "Cluster: 19 (25 docs)\n",
      "magnetic, magnetosphere, particle, particles, ring, inner, formalism, metrology, mems, relativistic\n",
      "\n",
      "Cluster: 20 (24 docs)\n",
      "commutative, algebras, algebra, operator, subfactors, matrices, quantum, harmonic, neumann, geometry\n",
      "\n",
      "Cluster: 21 (43 docs)\n",
      "customer, polymer, cad, sensor, power, supply, platform, xenon, reverse, platforms\n",
      "\n",
      "Cluster: 22 (18 docs)\n",
      "integrals, polynomials, theorem, functions, ergodic, polynomial, inequalities, regression, eigenvalues, permutations\n",
      "\n",
      "Cluster: 23 (31 docs)\n",
      "molecules, phosphorus, iron, corrosion, photodissociation, ion, clusters, clay, reactions, complexes\n",
      "\n",
      "Cluster: 24 (17 docs)\n",
      "printing, xerography, mixed, control, markov, financial, selection, screening, linear, manufacturing\n",
      "\n",
      "Cluster: 25 (18 docs)\n",
      "centrifuge, laminar, table, nied, shaking, box, seismic, 1g, rpi, soil\n",
      "\n",
      "Cluster: 26 (33 docs)\n",
      "abroad, fellowship, dr, twenty, ancient, netherlands, oxford, archaeological, exotic, amb\n",
      "\n",
      "Cluster: 27 (48 docs)\n",
      "superconductors, magnetic, superconducting, superfluid, polymer, superconductivity, quantum, condensed, nanoscale, spin\n",
      "\n",
      "Cluster: 28 (43 docs)\n",
      "algebraic, curves, varieties, arithmetic, equations, algebra, geometry, conjecture, galois, moduli\n",
      "\n",
      "Cluster: 29 (32 docs)\n",
      "3d, resistivity, void, cave, mineralogy, tumors, sinkholes, cardiac, ankle, shale\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation to choices\n",
    "\n",
    "I chose ngram range (1,2) because anything more than that would be rare and max limit is at base 20000 because more rare wont have too much of an effect on it\n",
    "\n",
    "Chose K-means because of the speed difference and the performance. cluster size dunno why yet, Kmeans more familiar with it than hierachical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b - Inspect the keywords of the clusters. List the 10 first clusters out of all (i.e., not cherry picked examples) and provide an as descriptive label as possible for each of them. \n",
    "\n",
    "\n",
    "NOTE PRINT 10 FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (4 docs)\n",
      "proofs, resultants, theorems, nova, lemma, footage, analogy, pbs, logarithmic, wgbh\n",
      "\n",
      "Cluster: 1 (88 docs)\n",
      "equations, dynamical, fluid, nonlinear, random, waves, solutions, fluids, mesoscale, schrodinger\n",
      "\n",
      "Cluster: 2 (46 docs)\n",
      "colleges, alliance, technicians, college, umeb, manufacturing, curriculum, workforce, technical, industry\n",
      "\n",
      "Cluster: 3 (43 docs)\n",
      "available, not, zygotic, zygomycota, zygomycetes, zygmund, zurich, zuni, zro2, zr\n",
      "\n",
      "Cluster: 4 (56 docs)\n",
      "conference, algebraic, contract, arsenic, seminar, meeting, stochastic, statistical, grid, haiwee\n",
      "\n",
      "Cluster: 5 (13 docs)\n",
      "oceanographic, vessel, fleet, ship, ships, shipboard, operated, ctd, unols, equipment\n",
      "\n",
      "Cluster: 6 (26 docs)\n",
      "fellowship, mathematical, sciences, fellowships, zygotic, zygomycota, zygomycetes, zygmund, zurich, zuni\n",
      "\n",
      "Cluster: 7 (30 docs)\n",
      "algebras, representation, lie, symmetries, theory, representations, finite, sheaves, quantum, automorphic\n",
      "\n",
      "Cluster: 8 (45 docs)\n",
      "workshop, cmes, costa, cme, sediment, solar, coronagraph, tectonically, subduction, arc\n",
      "\n",
      "Cluster: 9 (20 docs)\n",
      "microbial, fellowship, biology, bacterial, fungal, training, fungi, bacteria, host, virulence\n",
      "\n",
      "Cluster: 10 (19 docs)\n",
      "quantum, straightening, networks, required, no, neural, repair, abstract, algorithm, heat\n",
      "\n",
      "Cluster: 11 (44 docs)\n",
      "nigms, protein, panel, biology, rna, genes, transcription, packages, nucleic, duplication\n",
      "\n",
      "Cluster: 12 (49 docs)\n",
      "manifolds, spaces, curvature, conformal, geometry, equations, geometric, operators, operator, einstein\n",
      "\n",
      "Cluster: 13 (41 docs)\n",
      "teachers, teacher, mathematics, college, classroom, assistive, school, middle, classrooms, competencies\n",
      "\n",
      "Cluster: 14 (16 docs)\n",
      "fellowship, postdoctoral, he, informatics, redshift, galaxies, training, she, stars, astronomy\n",
      "\n",
      "Cluster: 15 (44 docs)\n",
      "optical, semiconductor, ultrabroadband, fiber, wavelength, plasma, ionosphere, dots, cavity, surfaces\n",
      "\n",
      "Cluster: 16 (10 docs)\n",
      "crcd, curriculum, entitled, notre, dame, communications, photovoltaic, pittsburgh, ethics, biomolecular\n",
      "\n",
      "Cluster: 17 (13 docs)\n",
      "czech, crystals, films, crystal, doped, cytokinin, homotopy, telluride, transparent, inorganic\n",
      "\n",
      "Cluster: 18 (61 docs)\n",
      "terrorist, tropical, attitudes, attacks, archaeological, instrument, harvard, metropolitan, russian, pacific\n",
      "\n",
      "Cluster: 19 (25 docs)\n",
      "magnetic, magnetosphere, particle, particles, ring, inner, formalism, metrology, mems, relativistic\n",
      "\n",
      "Cluster: 20 (24 docs)\n",
      "commutative, algebras, algebra, operator, subfactors, matrices, quantum, harmonic, neumann, geometry\n",
      "\n",
      "Cluster: 21 (43 docs)\n",
      "customer, polymer, cad, sensor, power, supply, platform, xenon, reverse, platforms\n",
      "\n",
      "Cluster: 22 (18 docs)\n",
      "integrals, polynomials, theorem, functions, ergodic, polynomial, inequalities, regression, eigenvalues, permutations\n",
      "\n",
      "Cluster: 23 (31 docs)\n",
      "molecules, phosphorus, iron, corrosion, photodissociation, ion, clusters, clay, reactions, complexes\n",
      "\n",
      "Cluster: 24 (17 docs)\n",
      "printing, xerography, mixed, control, markov, financial, selection, screening, linear, manufacturing\n",
      "\n",
      "Cluster: 25 (18 docs)\n",
      "centrifuge, laminar, table, nied, shaking, box, seismic, 1g, rpi, soil\n",
      "\n",
      "Cluster: 26 (33 docs)\n",
      "abroad, fellowship, dr, twenty, ancient, netherlands, oxford, archaeological, exotic, amb\n",
      "\n",
      "Cluster: 27 (48 docs)\n",
      "superconductors, magnetic, superconducting, superfluid, polymer, superconductivity, quantum, condensed, nanoscale, spin\n",
      "\n",
      "Cluster: 28 (43 docs)\n",
      "algebraic, curves, varieties, arithmetic, equations, algebra, geometry, conjecture, galois, moduli\n",
      "\n",
      "Cluster: 29 (32 docs)\n",
      "3d, resistivity, void, cave, mineralogy, tumors, sinkholes, cardiac, ankle, shale\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(matrix_sample, km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c - Select one or two good clusters (that can be clearly interpreted) and one or two bad clusters (that might be difficult to interpret or distinguish). Motivate your choise (clusters may, for instance, be overlapping, too broad/narrow or incoherent). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d - Repeat the experiment in (a) with LDA topic modeling instead (on the whole corpus), and explain briefly how the results compare to your previously chosen clustering setup. A few concrete examples may be helpful. Do your best to make sure the list of topic keywords are informative through appropriate post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic modeling demo\n",
    "#!pip3 install gensim\n",
    "\n",
    "# Fast and simple tokenization\n",
    "new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(abs) for abs in abstracts]\n",
    "\n",
    "# Train LDA model\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "0.0078\tprogram\n",
      "0.0072\tstudents\n",
      "0.0070\tresearch\n",
      "0.0064\tUniversity\n",
      "0.0055\tproject\n",
      "0.0050\tscience\n",
      "0.0043\tengineering\n",
      "0.0039\tThis\n",
      "0.0032\ttheir\n",
      "0.0031\tyear\n",
      "\n",
      "Topic 1\n",
      "0.0026\tretail\n",
      "0.0026\tThis\n",
      "0.0024\tMJO\n",
      "0.0021\tquery\n",
      "0.0019\tproject\n",
      "0.0018\tlake\n",
      "0.0017\tresearch\n",
      "0.0016\tdata\n",
      "0.0014\tnew\n",
      "0.0013\tthese\n",
      "\n",
      "Topic 2\n",
      "0.0058\tresearch\n",
      "0.0048\tproject\n",
      "0.0047\tThis\n",
      "0.0041\tthese\n",
      "0.0039\thave\n",
      "0.0034\tstudy\n",
      "0.0032\twhich\n",
      "0.0032\thas\n",
      "0.0029\tunderstanding\n",
      "0.0027\ttheir\n",
      "\n",
      "Topic 3\n",
      "0.0113\tspecies\n",
      "0.0064\tplant\n",
      "0.0059\tplants\n",
      "0.0056\tgenes\n",
      "0.0041\tresearch\n",
      "0.0041\tevolutionary\n",
      "0.0040\tThis\n",
      "0.0039\tthese\n",
      "0.0035\tgenetic\n",
      "0.0034\tgene\n",
      "\n",
      "Topic 4\n",
      "0.0079\tresearch\n",
      "0.0067\tmaterials\n",
      "0.0055\tThis\n",
      "0.0038\tnew\n",
      "0.0038\tproject\n",
      "0.0038\thigh\n",
      "0.0037\tproperties\n",
      "0.0034\tthese\n",
      "0.0031\tused\n",
      "0.0029\tstudents\n",
      "\n",
      "Topic 5\n",
      "0.0051\tdata\n",
      "0.0042\tThis\n",
      "0.0036\tstudy\n",
      "0.0029\thave\n",
      "0.0028\tthese\n",
      "0.0028\tresearch\n",
      "0.0027\tproject\n",
      "0.0025\twhich\n",
      "0.0025\tice\n",
      "0.0023\tbetween\n",
      "\n",
      "Topic 6\n",
      "0.0045\tcarbon\n",
      "0.0035\tresearch\n",
      "0.0035\tspecies\n",
      "0.0035\tthese\n",
      "0.0031\tbetween\n",
      "0.0030\tproject\n",
      "0.0030\tThis\n",
      "0.0028\tsystem\n",
      "0.0028\thave\n",
      "0.0028\tCO2\n",
      "\n",
      "Topic 7\n",
      "0.0125\tresearch\n",
      "0.0106\tstudents\n",
      "0.0048\tproject\n",
      "0.0048\tscience\n",
      "0.0045\tThis\n",
      "0.0042\ttheir\n",
      "0.0036\teducation\n",
      "0.0035\tlearning\n",
      "0.0033\tdata\n",
      "0.0029\tnew\n",
      "\n",
      "Topic 8\n",
      "0.0011\tresearch\n",
      "0.0006\tproject\n",
      "0.0006\tnew\n",
      "0.0005\twhich\n",
      "0.0005\tThis\n",
      "0.0004\thave\n",
      "0.0004\tstudents\n",
      "0.0003\tmaterials\n",
      "0.0003\tIn\n",
      "0.0003\tdata\n",
      "\n",
      "Topic 9\n",
      "0.0060\tresearch\n",
      "0.0058\tThis\n",
      "0.0047\tproject\n",
      "0.0041\tsystems\n",
      "0.0032\tnew\n",
      "0.0029\twhich\n",
      "0.0028\tsuch\n",
      "0.0027\thave\n",
      "0.0026\tthese\n",
      "0.0025\tapplications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda_model.show_topics(num_words=50, formatted=False):\n",
    "    print(\"Topic\", i)\n",
    "    printed_terms = 0\n",
    "    for term, score in topic:\n",
    "        if printed_terms >= 10:\n",
    "            break\n",
    "        elif term in \"the of and to for in or The is be may an a with at are on by as from can will that this\".split(): # to lower case?\n",
    "            continue\n",
    "        printed_terms += 1\n",
    "        print(\"%.4f\\t%s\" % (score,term))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
