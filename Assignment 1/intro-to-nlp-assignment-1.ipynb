{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Assignment 1 \n",
    "\n",
    "### Description of Assignment 1\n",
    "\n",
    "This assignment relates to Theme Basic NLP of the introduction to NLP (courses Deskriptiv analytik / Machine learning for descriptive problems), and will focus on basic text processing methods on Python.\n",
    "\n",
    "The assignment is handed in as a Jupyter notebook containing the code used to solve the problem, output presenting the results, and, most importantly, notes that present the students' conclusions and answer questions posed in the assignment. \n",
    "\n",
    "**Assignment steps/Questions:**\n",
    "\n",
    "Download and extract data package from here [http://dl.turkunlp.org/intro-to-nlp.tar.gz](http://dl.turkunlp.org/intro-to-nlp.tar.gz). Gzipped file intro-to-nlp/english-tweets-sample.jsonl.gz includes 10,000 English tweets downloaded from the Twiter API. The file is compressed and in JSON Lines format ([http://jsonlines.org/](http://jsonlines.org/)), i.e. one json per line.\n",
    "\n",
    "Note: If processing the whole fiel takes too long, it's okay to read just a subset of the data, for example only 2,000 tweets...\n",
    "\n",
    "1. Read tweets in Python \n",
    "2. Extract the actual text fields from the tweet jsons, discard all metadata at this point. Note that sometimes the text may be truncated to fit the old character limit. In these cases, is it possible to get the full text?\n",
    "3. Segment each tweet using both UDPipe machine learned model (can be found from the same data package) and a heuristic method. What can you tell about the segmentation performance on tweets when manually inspecting few examples?\n",
    "4. Count a word frequency list (how many times each word appears and how many unique words there are). Which are the most commmon words appearing in the data? What kind of words these are?\n",
    "5. Calculate **idf** weight for each word appearing in the data (one tweet = one document), and print top 20 words with lowest and highest idf values. Why **tf** not really matter when processing tweets?\n",
    "6. Find duplicate or near duplicate tweets (in terms of text field only) in the data using any method you see fit. What kind of techniques you considered using and/or tested, and how many duplicate or near duplicate did you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Niklas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "import ufal.udpipe as udpipe\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords') # download the stopwords dataset\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read tweets in Python \n",
    "\n",
    "Let's start by reading in the tweets from the gzipped jsonline file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from gzip file and decode the json to a list \n",
    "\n",
    "#import json\n",
    "#import gzip\n",
    "\n",
    "data = []\n",
    "with gzip.open('english-tweets-sample.jsonl.gz', 'rb')as f:\n",
    "    for line in f:        \n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10000\n",
      "Data type: <class 'list'>\n",
      "First item type: <class 'dict'>\n",
      "First item: {'created_at': 'Tue Dec 26 14:16:22 +0000 2017', 'id': 945659557480611840, 'id_str': '945659557480611840', 'text': 'Check out my class in #GranblueFantasy! https://t.co/pAvXn8diJr', 'display_text_range': [0, 39], 'source': '<a href=\"http://granbluefantasy.jp/\" rel=\"nofollow\">グランブルー ファンタジー</a>', 'truncated': False, 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 883980236655779840, 'id_str': '883980236655779840', 'name': 'Pc Kwok', 'screen_name': 'jensenpck', 'location': None, 'url': None, 'description': None, 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 0, 'friends_count': 1, 'listed_count': 0, 'favourites_count': 0, 'statuses_count': 42, 'created_at': 'Sun Jul 09 09:24:46 +0000 2017', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': 'zh-tw', 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png', 'profile_image_url_https': 'https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'quote_count': 0, 'reply_count': 0, 'retweet_count': 0, 'favorite_count': 0, 'entities': {'hashtags': [{'text': 'GranblueFantasy', 'indices': [22, 38]}], 'urls': [], 'user_mentions': [], 'symbols': [], 'media': [{'id': 945659555123404801, 'id_str': '945659555123404801', 'indices': [40, 63], 'media_url': 'http://pbs.twimg.com/media/DR-oWuWVoAEUZJd.jpg', 'media_url_https': 'https://pbs.twimg.com/media/DR-oWuWVoAEUZJd.jpg', 'url': 'https://t.co/pAvXn8diJr', 'display_url': 'pic.twitter.com/pAvXn8diJr', 'expanded_url': 'https://twitter.com/jensenpck/status/945659557480611840/photo/1', 'type': 'photo', 'sizes': {'small': {'w': 680, 'h': 340, 'resize': 'fit'}, 'thumb': {'w': 150, 'h': 150, 'resize': 'crop'}, 'medium': {'w': 1024, 'h': 512, 'resize': 'fit'}, 'large': {'w': 1024, 'h': 512, 'resize': 'fit'}}}]}, 'extended_entities': {'media': [{'id': 945659555123404801, 'id_str': '945659555123404801', 'indices': [40, 63], 'media_url': 'http://pbs.twimg.com/media/DR-oWuWVoAEUZJd.jpg', 'media_url_https': 'https://pbs.twimg.com/media/DR-oWuWVoAEUZJd.jpg', 'url': 'https://t.co/pAvXn8diJr', 'display_url': 'pic.twitter.com/pAvXn8diJr', 'expanded_url': 'https://twitter.com/jensenpck/status/945659557480611840/photo/1', 'type': 'photo', 'sizes': {'small': {'w': 680, 'h': 340, 'resize': 'fit'}, 'thumb': {'w': 150, 'h': 150, 'resize': 'crop'}, 'medium': {'w': 1024, 'h': 512, 'resize': 'fit'}, 'large': {'w': 1024, 'h': 512, 'resize': 'fit'}}}]}, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'filter_level': 'low', 'lang': 'en', 'timestamp_ms': '1514297782665'}\n"
     ]
    }
   ],
   "source": [
    "# Show information of the data\n",
    "print(\"Number of documents:\", len(data))\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"First item type:\", type(data[0]))\n",
    "print(\"First item:\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract the actual text fields from the tweet jsons\n",
    "\n",
    "**Extract the actual text fields from the tweet jsons, discard all metadata at this point. Note that sometimes the text may be truncated to fit the old character limit. In these cases, is it possible to get the full text?**\n",
    "\n",
    "Let's start by extracting only the text fields from the tweets. Some of the tweets are truncated and yes it is possible to extract the full text from truncated tweets. \n",
    "\n",
    "There are two kind of truncated texts:\n",
    "1. Original tweets that are truncated. (Value of truncated == True)\n",
    "2. Retweeted tweets that are truncated. \n",
    "\n",
    "The retweeted tweets do not say from the truncated status if they are truncated. From the retweeted_status it is possible to check if the original tweet has been truncated and to extract the full_text tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the actual text field an ddiscard meta data\n",
    "documents = []\n",
    "\n",
    "for d in data:\n",
    "    # Check if retweeted\n",
    "    if(\"retweeted_status\" in d):\n",
    "        # Check if retweet is truncated\n",
    "        if(d[\"retweeted_status\"][\"truncated\"] == True):\n",
    "            documents.append(d[\"retweeted_status\"][\"extended_tweet\"][\"full_text\"])    \n",
    "        else:\n",
    "            documents.append(d[\"retweeted_status\"][\"text\"])\n",
    "    # Check if original tweet is truncated                \n",
    "    elif(d[\"truncated\"] == True): \n",
    "        documents.append(d[\"extended_tweet\"][\"full_text\"])\n",
    "    # Default case\n",
    "    else:\n",
    "        documents.append(d[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10000\n",
      "Documents type: <class 'list'>\n",
      "First item type: <class 'str'>\n",
      "First item: Check out my class in #GranblueFantasy! https://t.co/pAvXn8diJr\n",
      "Second item: Extending a big Thank You to our Community Partner all over the world! https://t.co/cu7on7g1si\n",
      "Third item: Blueberry 🍨 https://t.co/2gzHAFWYJY\n"
     ]
    }
   ],
   "source": [
    "# Show information of the data\n",
    "print(\"Number of documents:\", len(documents))\n",
    "print(\"Documents type:\", type(documents))\n",
    "print(\"First item type:\", type(documents[0]))\n",
    "print(\"First item:\", documents[0])\n",
    "print(\"Second item:\", documents[1])\n",
    "print(\"Third item:\", documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Segment each tweet using both UDPipe machine learned model and a heuristic method\n",
    "\n",
    "**Segment each tweet using both UDPipe machine learned model (can be found from the same data package) and a heuristic method. What can you tell about the segmentation performance on tweets when manually inspecting few examples?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation with UDPipe machine learned model\n",
    "\n",
    "#import ufal.udpipe as udpipe\n",
    "\n",
    "model = udpipe.Model.load(\"en.segmenter.udpipe\")\n",
    "pipeline = udpipe.Pipeline(model,\"tokenize\",\"none\",\"none\",\"horizontal\")\n",
    "\n",
    "segmented_documents = []\n",
    "\n",
    "for d in documents:\n",
    "    segmented_documents.append(pipeline.process(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check out my class in # GranblueFantasy !\\nhtt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extending a big Thank\\nYou to our Community Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blueberry 🍨 https://t.co/2gzHAFWYJY\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bad day ☹️®️\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@prologve_ @BTS_ARMY @BTS_twt I 'm Chim tho\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0  Check out my class in # GranblueFantasy !\\nhtt...\n",
       "1  Extending a big Thank\\nYou to our Community Pa...\n",
       "2              Blueberry 🍨 https://t.co/2gzHAFWYJY\\n\n",
       "3                                     Bad day ☹️®️\\n\n",
       "4      @prologve_ @BTS_ARMY @BTS_twt I 'm Chim tho\\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data\n",
    "df = pd.DataFrame({'Tweets':segmented_documents})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation with a heuristic model\n",
    "\n",
    "#import re\n",
    "\n",
    "heuristic_seg_docs = []\n",
    "\n",
    "for d in documents:\n",
    "    segmented = re.sub(r'https?:\\/\\/.*\\/\\w*', '', d) #remove links\n",
    "    segmented = re.sub(r'@[^\\s]+', '', segmented) #remove twitter handles\n",
    "    segmented = re.sub(r'#[^\\s]+', '', segmented) #remove hashtags\n",
    "    segmented = re.sub(r'(&.+;)', '', segmented) #remove &amp;  &gt;  ...\n",
    "    segmented = re.sub(r'[\\U00010000-\\U0010ffff]', '', segmented) #remove some emojis\n",
    "    segmented = segmented.lower() # Make every char lower for easier tokenization\n",
    "    segmented = re.sub(r'rt', '', segmented) #remove retweets\n",
    "    segmented = re.sub(r'([. , ! ? : ; # @ & - € $]+)', r' \\1 ', segmented) # replace \n",
    "    segmented = re.sub(r\"(’t)\", r\" \\1\", segmented) # clitics n’t\n",
    "    segmented = re.sub(r\"('t)\", r\" \\1\", segmented) # clitics n't\n",
    "    segmented = re.sub(r\"(’s)\", r\" \\1\", segmented) # clitics ’s\n",
    "    segmented = re.sub(r\"('s)\", r\" \\1\", segmented) # clitics 's\n",
    "    segmented = re.sub(r\"(’re)\", r\" \\1\", segmented) # clitics ’re\n",
    "    segmented = re.sub(r\"('re)\", r\" \\1\", segmented) # clitics 're\n",
    "    segmented = re.sub(r\"(’m)\", r\" \\1\", segmented) # clitics ’m\n",
    "    segmented = re.sub(r\"('m)\", r\" \\1\", segmented) # clitics 'm\n",
    "    segmented = re.sub(r\"(’ve)\", r\" \\1\", segmented) # clitics ’ve\n",
    "    segmented = re.sub(r\"('ve)\", r\" \\1\", segmented) # clitics ve\n",
    "    segmented = re.sub(r\"(’d)\", r\" \\1\", segmented) # clitics ’d\n",
    "    segmented = re.sub(r\"('d)\", r\" \\1\", segmented) # clitics 'd\n",
    "    segmented = re.sub(r\"(’ll)\", r\" \\1\", segmented) # clitics ’ll\n",
    "    segmented = re.sub(r\"('ll)\", r\" \\1\", segmented) # clitics 'll\n",
    "    segmented = re.sub(r'\\s+', ' ',   segmented) # Remove duplicate whitespaces*\n",
    "    \n",
    "    heuristic_seg_docs.append(segmented)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>check out my class in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>extending a big thank you to our community pan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blueberry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad day ☹️®️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i 'm chim tho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets\n",
       "0                             check out my class in \n",
       "1  extending a big thank you to our community pan...\n",
       "2                                         blueberry \n",
       "3                                       bad day ☹️®️\n",
       "4                                      i 'm chim tho"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data\n",
    "df = pd.DataFrame({'Tweets':heuristic_seg_docs})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results off the UDPipe and the heuristic segmentation are quite different. The heuristic method works pretty good in English atleast. You have to take into consideration the language specific rules. The links and twitter handles are easy to remove with a few regexes. If the heuristic method gets slang or badly written english it might do some weird segmentations, ex. i'mma -> \" i 'mma \". The heuristic model can target more specific segmentation needs, but also requires a lot of work to do it propperly. I prefered the use of the heuristic method because of the ability to easily remove all the hashtags, twitter handles, links and other trash in the text.\n",
    "\n",
    "Comparing the first tweets shows that the links and hashtags have dissapeared in the heuristic method. Other wise they have pretty similar results.\n",
    "\n",
    "The UDPipe machine learned model works very well. It takes into account clitics and hyphenated compound words.\n",
    "The UDPipe can be trained and work with several languages. It seems to be a very good and fast way of segmenting words if you have a trained classifier. This seems to be the optimal way of segmenting texts if you want to do it fast and well. \n",
    "\n",
    "The both methods seem to have done a good job, but to truly meassure the performance of the two sgementation method you could meassue the amount of words that are correctly segmented. I will use the heurisitc method in the following calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Count a word frequency list \n",
    "\n",
    "**Count a word frequency list (how many times each word appears and how many unique words there are). Which are the most commmon words appearing in the data? What kind of words these are?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word frequency counter\n",
    "\n",
    "#from collections import Counter\n",
    "\n",
    "token_counter = Counter()\n",
    "\n",
    "for doc in heuristic_seg_docs: \n",
    "    tokenized = pipeline.process(doc)\n",
    "    tokens = tokenized.split() # after segmenter, we can do whitespace splitting\n",
    "    token_counter.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 17187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>.</td>\n",
       "      <td>5929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the</td>\n",
       "      <td>4345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>,</td>\n",
       "      <td>3714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to</td>\n",
       "      <td>3445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>2940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i</td>\n",
       "      <td>2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>and</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>you</td>\n",
       "      <td>2663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>of</td>\n",
       "      <td>2069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>for</td>\n",
       "      <td>1858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>is</td>\n",
       "      <td>1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>:</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>it</td>\n",
       "      <td>1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>!</td>\n",
       "      <td>1418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-</td>\n",
       "      <td>1407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>this</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>on</td>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>that</td>\n",
       "      <td>1158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "69      .   5929\n",
       "16    the   4345\n",
       "92      ,   3714\n",
       "10     to   3445\n",
       "6       a   2940\n",
       "23      i   2871\n",
       "78    and   2705\n",
       "9     you   2663\n",
       "63     of   2069\n",
       "4      in   1945\n",
       "37    for   1858\n",
       "286    is   1765\n",
       "33      :   1595\n",
       "88     it   1531\n",
       "18      !   1418\n",
       "41      -   1407\n",
       "74   this   1248\n",
       "138    on   1217\n",
       "210  that   1158\n",
       "2      my   1119"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data\n",
    "print(\"Vocabulary size:\", len(token_counter))\n",
    "df = pd.DataFrame.from_dict(token_counter, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'Word', 0:'Count'})\n",
    "df.sort_values('Count', ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vocabulary size of the segmented documents is 17187. \n",
    "Before cleaning the tokens, the most common words seem to be punctuation charachters and stop words. \n",
    "\n",
    "Let's clean the data a bit and then take a closer look. Let's remove some of the stop words and punctuation characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation characters\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords') # download the stopwords dataset\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "filtered_tokens = {}\n",
    "punctuation_chars = '. .. , : ( ) ! !! ? ?? \" = & - ; ... \\\\ \" ” [ ] # @ “ / * % € $ '.split() # list of punctuation symbols to ignore\n",
    "for word, count in token_counter.most_common():\n",
    "    if word.lower() in stopwords.words(\"english\") or word in punctuation_chars:\n",
    "        continue\n",
    "    filtered_tokens[word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 17014\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>’s</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>christmas</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'t</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>’t</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>one</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>’</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>love</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>people</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>new</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>year</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>get</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>day</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>good</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>today</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Count\n",
       "0          's    929\n",
       "1          ’s    642\n",
       "2   christmas    579\n",
       "3          't    569\n",
       "4        like    532\n",
       "5          ’t    474\n",
       "6         one    456\n",
       "7           ’    447\n",
       "8        love    438\n",
       "9      people    396\n",
       "10        new    379\n",
       "11       year    345\n",
       "12        get    344\n",
       "13        day    339\n",
       "14          '    339\n",
       "15       2017    304\n",
       "16       good    297\n",
       "17       time    296\n",
       "18      today    262\n",
       "19       2018    259"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data\n",
    "print(\"Vocabulary size:\", len(filtered_tokens))\n",
    "df = pd.DataFrame.from_dict(filtered_tokens, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'Word', 0:'Count'})\n",
    "df.sort_values('Count', ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stop words, some punctuation the vocabulary size changed to 17014 which is only 173 tokens removed.\n",
    "\n",
    "The vocabulary still has some emoticons/emojis/dingbats/symbols etc. We will ignore those for now.\n",
    "\n",
    "There are 17014 unique \"words\". As mentioned above, there is still some unwanted elements in the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most common words\n",
    "\n",
    "The some of most common words are the: \n",
    "* 's clitic \n",
    "* Christmas\n",
    "* 't clitic\n",
    "* like\n",
    "* one\n",
    "* love\n",
    "* people\n",
    "* new\n",
    "* year\n",
    "* get\n",
    "* day \n",
    "* 2017\n",
    "* good\n",
    "* time\n",
    "* today\n",
    "\n",
    "If we only look at the words, not numbers or clitics then we have:\n",
    "* Christmas\n",
    "* like\n",
    "* one\n",
    "* love\n",
    "* people\n",
    "* new\n",
    "* year\n",
    "* get\n",
    "* day \n",
    "* good\n",
    "* time\n",
    "* today\n",
    "\n",
    "### What kind of words are these?\n",
    "\n",
    "**The words are mostly Nouns** with a few verbs and adjectives in the mix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>’s</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>christmas</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'t</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>’t</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>one</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>’</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>love</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>people</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>new</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>year</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>get</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>day</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>good</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>today</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>see</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>know</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>u</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>need</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>got</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>family</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>life</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>want</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Count\n",
       "0          's    929\n",
       "1          ’s    642\n",
       "2   christmas    579\n",
       "3          't    569\n",
       "4        like    532\n",
       "5          ’t    474\n",
       "6         one    456\n",
       "7           ’    447\n",
       "8        love    438\n",
       "9      people    396\n",
       "10        new    379\n",
       "11       year    345\n",
       "12        get    344\n",
       "13        day    339\n",
       "14          '    339\n",
       "15       2017    304\n",
       "16       good    297\n",
       "17       time    296\n",
       "18      today    262\n",
       "19       2018    259\n",
       "20        see    251\n",
       "21       know    244\n",
       "22          1    235\n",
       "23          u    234\n",
       "24       need    232\n",
       "25        got    229\n",
       "26     family    228\n",
       "27          2    224\n",
       "28       life    224\n",
       "29       want    222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 30 most common words \n",
    "df.sort_values('Count', ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate **idf** weight for each word appearing in the data \n",
    "**Calculate idf weight for each word appearing in the data (one tweet = one document), and print top 20 words with lowest and highest idf values. Why tf not really matter when processing tweets?**\n",
    "\n",
    "Let's start by creating the TF and IDF functions. After that let's calculate the tf and idfs from the documents. The tf and idf only has words that are found in the filtered tokens list. \n",
    "\n",
    "I will use filtered_tokens for the IDF values. The list only contains words that are found in the filtered_tokens. I only want to show the IDF of the \"meaningful\" words and not the stop words. If I would use all the words in the documents, it should not affect the result all that much because of IDF. The more frequent a word is, the lower the value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF for all\n",
    "# Puts the words in to a dictionary containing dictionaries of all the words in that document   \n",
    "def calculateTF(documents):\n",
    "    wordDictionary = {}\n",
    "\n",
    "    tfDocuments = {}\n",
    "    key = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "    \n",
    "        docWordDictionary = {}\n",
    "        # Split words, then add them to a dictionary\n",
    "        bagOfWords = doc.split(\" \")\n",
    "        for word in bagOfWords:\n",
    "            if word in docWordDictionary:\n",
    "                docWordDictionary[word] += 1\n",
    "            else:\n",
    "                docWordDictionary[word] = 1\n",
    "\n",
    "        # Append document and incremet dictionary key\n",
    "        tfDocuments[key] = docWordDictionary\n",
    "        key += 1\n",
    "    \n",
    "    return tfDocuments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Idf for all words\n",
    "# Takes in a TF dictionary and returns a dictionary with words and their idf values\n",
    "def calculateIDF(documents):\n",
    "    import math\n",
    "    n = len(documents)\n",
    "    \n",
    "    idf = {}\n",
    "    \n",
    "    # loop through all words (every word is unique, count of word is not taken into account). \n",
    "    # If word is in filtered_tokens then add it. \n",
    "    for key, value in documents.items():\n",
    "        for word in value:\n",
    "            if(word in filtered_tokens):\n",
    "                if word in idf:\n",
    "                    idf[word] += 1\n",
    "                else:\n",
    "                    idf[word] = 1\n",
    "\n",
    "    # Convert count to idf with log(n/count)\n",
    "    for word, count in idf.items():\n",
    "            idf[word] = math.log(n/float(count))\n",
    "\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates TF and IDF values\n",
    "tfDocuments = calculateTF(heuristic_seg_docs)\n",
    "idfValues = calculateIDF(tfDocuments)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16162</th>\n",
       "      <td>fridge</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449</th>\n",
       "      <td>booing</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11546</th>\n",
       "      <td>cinderella</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11545</th>\n",
       "      <td>spotlight</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11544</th>\n",
       "      <td>coolest</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11542</th>\n",
       "      <td>grimey</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>shirahoshi</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6436</th>\n",
       "      <td>consumer</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>minimalist</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>yeahhh</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>biennale</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>cannes</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>helpline</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6443</th>\n",
       "      <td>pakistanis</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11532</th>\n",
       "      <td>depoing</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6446</th>\n",
       "      <td>achilles</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>heel</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547</th>\n",
       "      <td>medicine</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>correlation</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11548</th>\n",
       "      <td>wounded</td>\n",
       "      <td>9.21034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word      idf\n",
       "16162       fridge  9.21034\n",
       "6449        booing  9.21034\n",
       "11546   cinderella  9.21034\n",
       "11545    spotlight  9.21034\n",
       "11544      coolest  9.21034\n",
       "11542       grimey  9.21034\n",
       "11540   shirahoshi  9.21034\n",
       "6436      consumer  9.21034\n",
       "11539   minimalist  9.21034\n",
       "11538       yeahhh  9.21034\n",
       "11537     biennale  9.21034\n",
       "11536       cannes  9.21034\n",
       "6442      helpline  9.21034\n",
       "6443    pakistanis  9.21034\n",
       "11532      depoing  9.21034\n",
       "6446      achilles  9.21034\n",
       "6447          heel  9.21034\n",
       "11547     medicine  9.21034\n",
       "6428   correlation  9.21034\n",
       "11548      wounded  9.21034"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data, Highest idf values\n",
    "df = pd.DataFrame.from_dict(idfValues, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'Word', 0:'idf'})\n",
    "df.sort_values('idf', ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>'s</td>\n",
       "      <td>2.544657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>’s</td>\n",
       "      <td>2.915074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>christmas</td>\n",
       "      <td>2.945039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>like</td>\n",
       "      <td>3.036554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>'t</td>\n",
       "      <td>3.042824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>’t</td>\n",
       "      <td>3.184474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>one</td>\n",
       "      <td>3.186893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>love</td>\n",
       "      <td>3.267541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>people</td>\n",
       "      <td>3.352407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>new</td>\n",
       "      <td>3.381395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>get</td>\n",
       "      <td>3.426515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>year</td>\n",
       "      <td>3.493313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>day</td>\n",
       "      <td>3.513247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>good</td>\n",
       "      <td>3.589940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>time</td>\n",
       "      <td>3.649659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017</td>\n",
       "      <td>3.676951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>today</td>\n",
       "      <td>3.684887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>see</td>\n",
       "      <td>3.746509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>know</td>\n",
       "      <td>3.798694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>life</td>\n",
       "      <td>3.872802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word       idf\n",
       "28           's  2.544657\n",
       "56           ’s  2.915074\n",
       "30    christmas  2.945039\n",
       "150        like  3.036554\n",
       "160          't  3.042824\n",
       "60           ’t  3.184474\n",
       "115         one  3.186893\n",
       "420        love  3.267541\n",
       "63       people  3.352407\n",
       "296         new  3.381395\n",
       "103         get  3.426515\n",
       "364        year  3.493313\n",
       "10          day  3.513247\n",
       "445        good  3.589940\n",
       "458        time  3.649659\n",
       "87         2017  3.676951\n",
       "1230      today  3.684887\n",
       "463         see  3.746509\n",
       "623        know  3.798694\n",
       "259        life  3.872802"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data, Lowest idf values\n",
    "df = pd.DataFrame.from_dict(idfValues, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'Word', 0:'idf'})\n",
    "df.sort_values('idf', ascending=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency does not really matter because of the lenght and language used in twitter. If we would calculate the TF of the unfiltered documents, there would be alot of stop words and other trash bloating the calculations. Term Frequency in itself does not explain alot about the meaning of the document. If the user repeats a word often in a tweet it will also have a skewed TF value. Term frequency in itself is not a great method of processing tweets, but when combined with IDF we can get the TF-IDF values.\n",
    "\n",
    "IDF takes into account how often the word is present accros all the documents. The more it is present, the lower value it will have. This will filter better out the stop words and frequently used words. \n",
    "\n",
    "Depending on the task, it is wrong to asume that the stop words are pure noise in the data, but in this exercise I decided to remove them from the IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. BONUS: Find duplicate or near duplicate tweets\n",
    "**Find duplicate or near duplicate tweets (in terms of text field only) in the data using any method you see fit. What kind of techniques you considered using and/or tested, and how many duplicate or near duplicate did you find?**\n",
    "\n",
    "I though first about a very slow brute force method of comparing the strings. You could remove the spaces and only comapre the coherent string that is left. This method would find the \"EXACT\" match of a string. \n",
    "\n",
    "Next I thought about creating a matrix of the TF values then comparing one TF clolumn with the next. This is also one way to get the near duplicate tweets by words in the text. This method does not care about the word order. \n",
    "\n",
    "Lastly I remembered something called cosine similarity from math lectures in Aalto. After a bit of googling I found out a way to use TF-IDF to calculate the cosine similarity. So let's turn the matrix of TF-IDF values to vectors and calculate the similarity. This method should be able to find the duplicates and near duplicates because it gives a similarity measure. \n",
    "\n",
    "Let's start by trying to implement the TF-IDF and then calculate the cosine-similarity.\n",
    "\n",
    "I'm not entirely sure if i should calculate the TF-IDF with all the stop words but in this case i have only used words that are filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTFIDF(tf, idf):\n",
    "    \n",
    "    tfidf = {}\n",
    "    key = 0\n",
    "    \n",
    "    # create TF-IDF dictionaries of all the words, Will be used as vectors later with cosine similarity\n",
    "    for key, value in tf.items():\n",
    "        docTfIdf = dict(filtered_tokens)\n",
    "        docTfIdf = dict.fromkeys(docTfIdf, 0)\n",
    "        \n",
    "        # Convert TF to TF score with Frequency/total words\n",
    "        totalWordsInDoc = sum(value.values())\n",
    "        for word in value:\n",
    "            if(word in filtered_tokens):\n",
    "                docTfIdf[word] = value[word]/totalWordsInDoc * idf[word]\n",
    "                \n",
    "                \n",
    "       \n",
    "        tfidf[key] = docTfIdf\n",
    "        key += 1\n",
    "        \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TFIDF\n",
    "tfIdfValues = calculateTFIDF(tfDocuments,idfValues)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCosSim(tfidf):\n",
    "    \n",
    "    cosSim = {}\n",
    "    key = 0\n",
    "    \n",
    "    # create TF-IDF dictionaries of all the words, Will be used as vectors later with cosine similarity\n",
    "    for key, value in tfidf.items():\n",
    "        similarity = {}\n",
    "        key2 = 0\n",
    "        for compareKey, compareValue in tfidf.items():\n",
    "            if(compareKey <= key):\n",
    "                key2 += 1\n",
    "                pass\n",
    "            else:\n",
    "                sim = CosineSimilarity(value, compareValue)\n",
    "                # If similarity is above 90, the tweet is neear duplicate and if it is 1 it is a duplicate tweet\n",
    "                if(sim > 0.9):\n",
    "                    #print(sim)\n",
    "                    similarity[key2] = sim\n",
    "                key2 += 1\n",
    "        \n",
    "        #cosSim[key] = similarity\n",
    "        key += 1\n",
    "        #print(key)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return cosSim\n",
    "            \n",
    "            \n",
    "def CosineSimilarity (d1, d2):\n",
    "    d1List = list(d1.values())\n",
    "    d2List = list(d2.values())\n",
    "                  \n",
    "    dotProd = np.dot(d1List, d2List)\n",
    "    \n",
    "    d1Squared = [i*i for i in d1List]\n",
    "    d2Squared = [i*i for i in d2List]\n",
    "    d1Sqaured = np.sqrt(sum(d1Squared))\n",
    "    d2Sqaured = np.sqrt(sum(d2Squared))\n",
    "    denominator = d1Sqaured * d2Sqaured\n",
    "    \n",
    "    similarity = dotProd/denominator\n",
    "    \n",
    "    return similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niklas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4f8836101e2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Calculate cosine similarity dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mCosineSimDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateCosSim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfIdfValues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-e164b99b0b44>\u001b[0m in \u001b[0;36mcalculateCosSim\u001b[1;34m(tfidf)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCosineSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompareValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;31m# If similarity is above 90, the tweet is neear duplicate and if it is 1 it is a duplicate tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-e164b99b0b44>\u001b[0m in \u001b[0;36mCosineSimilarity\u001b[1;34m(d1, d2)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0md2List\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mdotProd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1List\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2List\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0md1Squared\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md1List\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity dictionary\n",
    "CosineSimDict = calculateCosSim(tfIdfValues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running this function on a single thread would take ages\n",
    "\n",
    "I began to read articles about speeding machine learning and data scinece methods in Python. I came across a library called numba that would enable me to use Cuda cores for processing. After toying around with cuda I tried to apply it to this project. I found out fast that the topic is more complex and needs more hours of reading the documentation to understand it. I got stuck with the np.sum(), dictionaries and other kinds of problems. I still wanted to represent som data so i run this code in Notebook CSC for the first 100 entires and compared them to all documents. I found out that in the first 100 there were:\n",
    "\n",
    "* 130 duplicates \n",
    "* 4 near duplicates\n",
    "\n",
    "In the future I am intrigued to learn more about splitting the processing workload to the GPU.\n",
    "\n",
    "When inspecting the examples of the duplicates i noticed that this method should be run with all the words in it if you want to have more accurate representation of duplicate tweets. Some of the tweets have small variations that are droped out by the heuristic method of segmenting the data or then by dropping the stop words. The texts context are almost the same when running this method. \n",
    "\n",
    "Below are the duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in array and prints all the documents based on keys from array\n",
    "def printDuplicateTweets(numbers):\n",
    "    for x in numbers:\n",
    "        print(heuristic_seg_docs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check out my class in \n",
      "check out my class in \n",
      "check out my class in \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([0,4352,9765])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad day ☹️®️\n",
      "bad day ☹️®️\n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([3,4241])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cases/application areas of in offline/ \n",
      "use cases/application areas of in offline/ \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([10,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our dad passed away earlier this summer so my mom and i decided to surprise my sisters with bears with his favorite cologne and a recording of his voice . it ’s not christmas without you dad , but we have you in spirit ❤️ \n",
      "our dad passed away earlier this summer so my mom and i decided to surprise my sisters with bears with his favorite cologne and a recording of his voice . it ’s not christmas without you dad , but we have you in spirit ❤️ \n",
      "our dad passed away earlier this summer so my mom and i decided to surprise my sisters with bears with his favorite cologne and a recording of his voice . it ’s not christmas without you dad , but we have you in spirit ❤️ \n",
      "our dad passed away earlier this summer so my mom and i decided to surprise my sisters with bears with his favorite cologne and a recording of his voice . it ’s not christmas without you dad , but we have you in spirit ❤️ \n",
      "our dad passed away earlier this summer so my mom and i decided to surprise my sisters with bears with his favorite cologne and a recording of his voice . it ’s not christmas without you dad , but we have you in spirit ❤️ \n",
      "our dad passed away earlier this summer so my mom and i decided to surprise my sisters with bears with his favorite cologne and a recording of his voice . it ’s not christmas without you dad , but we have you in spirit ❤️ \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([12,378,4474,4617,4620,8505])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president trump cuts funding to un after israel vote - newsweek \n",
      "president trump cuts funding to un after israel vote - newsweek \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([15,1128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n",
      "y’all could ’ve just said that a transgender couple have a baby rather than giving me brain damage \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([26,1094,1317,1391,2072,2138,3563,5382,5779,6225,6652,8530,8794,9809,9957])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tron is a long term hodl . those who are flipping it are losing out . \n",
      "tron is a long term hodl . those who are flipping it are losing out . \n",
      "tron is a long term hodl . those who are flipping it are losing out . \n",
      "tron is a long term hodl . those who are flipping it are losing out . \n",
      "tron is a long term hodl . those who are flipping it are losing out . \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([30,1563,4142,4161,8557])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " itranslategames lady gaga ☂ zara larsson\n",
      " itranslategames lady gaga ☂ zara larsson\n",
      " itranslategames lady gaga ☂ zara larsson\n",
      " itranslategames lady gaga ☂ zara larsson\n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([35,1255,6410,6981])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n",
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n",
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n",
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n",
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n",
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n",
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n",
      "my lil filipino mom thought her iphone x was perfume and i cry everytime i watch it . \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([40,177,1098,4234,4861,7270,9306,9977])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n",
      "george lopez my wife and kids everybkdy hates chris the nanny fresh prince of belair rick moy degrassi law order the big bang theory sabrina the teenage witch full house boy meets world scooby-doo , where are you ? all that drake josh thats so raven even stevens zoey 101 \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([42,409,2903,4475,5515,6083,6885,8670,9098,9825])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n",
      "it ’s a new day and a new world , or so it seems . yesterday ’s pr ... more for aries \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([43,1320,3233,3263,3485,3647,4509,6941,8705,9154,9187,9594,9880,9994])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nypl_tweeter lady gaga ☂ zara larsson\n",
      " nypl_tweeter lady gaga ☂ zara larsson\n",
      " nypl_tweeter lady gaga ☂ zara larsson\n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([44,3304,4003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss you \n",
      "i miss you\n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([46,6011])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if you knew what my celebration was \n",
      " if you knew what my celebration was \n",
      " if you knew what my celebration was \n",
      " if you knew what my celebration was \n",
      " if you knew what my celebration was \n",
      " if you knew what my celebration was \n",
      " if you knew what my celebration was \n",
      " if you knew what my celebration was \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([47,789,1790,3814,4136,4138,4358,6170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lmao this is amazing still funny \n",
      " lmao this is amazing still funny \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([48,2748])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this year , trump has spent $ 91 , 655 , 424 of american taxpayer money on golf trips . if you are struggling now , to buy present , pay your heat bill , buy medication , or anything else , just remember this one simple fact . trump does not care about you . \n",
      "this year , trump has spent $ 91 , 655 , 424 of american taxpayer money on golf trips . if you are struggling now , to buy present , pay your heat bill , buy medication , or anything else , just remember this one simple fact . trump does not care about you . \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([55,9753])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear twits i wish you all a happy christ 's bihday anniversary , regardless of your culture , class , religious orientation , sexual preference , or race , provided only that you don 't vote next year for the delusional five-year-old currently destroying everything i love about the usa\n",
      "dear twits i wish you all a happy christ 's bihday anniversary , regardless of your culture , class , religious orientation , sexual preference , or race , provided only that you don 't vote next year for the delusional five-year-old currently destroying everything i love about the usa\n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([58,2604])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that ’s a big mood \n",
      "big mood \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([60,9217])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 people followed me and one person unfollowed me // automatically checked by \n",
      "one person followed me and 3 people unfollowed me // automatically checked by \n",
      "10 people followed me and one person unfollowed me // automatically checked by \n",
      "2 people followed me and one person unfollowed me // automatically checked by \n",
      "2 people followed me and one person unfollowed me // automatically checked by \n",
      "one person followed me and 3 people unfollowed me // automatically checked by \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([61,156,651,997,5173,8953])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no more “my bad i didn ’t see your call” in 2018 . i seen it , i ignored it . \n",
      "no more “my bad i didn ’t see your call” in 2018 . i seen it , i ignored it . \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([65,4656])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear god , thank you \n",
      "dear god , thank you \n",
      "dear god , thank you \n",
      "dear god , thank you \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([74,75,4208,5177])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear god , thank you \n",
      "dear god , thank you \n",
      "dear god , thank you \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([75,4208,5177])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is one of my fav looks on yoongi \n",
      "this is one of my fav looks on yoongi \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([82,4749])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n",
      "an unfinished discussion with a family member may unexpectedly ... more for virgo \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([86,115,301,1003,2171,2199,2912,3616,3648,5024,5049,5998,6577,6936,7458,8506,8958,9087,9625,9800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "their christmas tree \n",
      "their christmas tree \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([91,6714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a festival created by prof maulana karenga he was convicted of beating women , after removing their clothes . they were also burned with hot soldering irons , while detergent running hoses were forced down in their throats celebrate celebrate abuse \n",
      " a festival created by prof maulana karenga he was convicted of beating women , after removing their clothes . they were also burned with hot soldering irons , while detergent running hoses were forced down in their throats celebrate celebrate abuse \n",
      " a festival created by prof maulana karenga he was convicted of beating women , after removing their clothes . they were also burned with hot soldering irons , while detergent running hoses were forced down in their throats celebrate celebrate abuse \n"
     ]
    }
   ],
   "source": [
    "printDuplicateTweets([98,5315,8282])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
