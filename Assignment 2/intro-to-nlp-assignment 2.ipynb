{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP: Assignment 2 \n",
    "\n",
    "## Assignment on Text Classification and Sequence Labeling\n",
    "\n",
    "### Description of Assignment 2\n",
    "\n",
    "This assignment relates to the Text classification and Sequence labeling themes of the introduction to NLP (courses Deskriptiv analytik / Machine learning for descriptive problems), and will focus on gaining some practical, hands-on experience in building and training simple models for these tasks.\n",
    "\n",
    "The assignment is handed in as a Jupyternotebook (or a PDF render thereof) containing the code used to solve the problem, output presenting the results, and, most importantly, notes that present the students' conclusions and answer questions posed in the assignment.\n",
    "\n",
    "**Assignment steps/Questions:**\n",
    "\n",
    "1. Test sklearn’s TfidfVectorizer in place of CountVectorizer on the IMDB data. Do you see any difference in the classification results or the optimal C value?\n",
    "\n",
    "2. Test different lengths of n-grams in the CountVectorizer on the IMDB data. Do you see any difference in the classification results or the optimal C value ? Do these n-grams show up also in the list of most significant positive/negative features?\n",
    "\n",
    "3. In the data package for the course [http://dl.turkunlp.org/intro-to-nlp.tar.gz](http://dl.turkunlp.org/intro-to-nlp.tar.gz), the directory language_identification contains data for 5 languages. Based on this data, train an SVM classifier for language recognition between these 5 languages.\n",
    "\n",
    "4. If you completed (3), toy around with features, especially the ngram_range and analyzer parameters, which allow you to test classification based on character ngrams of various lengths (not only word n-grams). Gain some insight in to the accuracy of the classifier with different features, and try to identify misclassified documents -why do you think they were misclassified?\n",
    "\n",
    "5. **BONUS** On the address universaldependencies.org, you will find datasets for a bunch of languages. These come in an easy-to-parse, well-documented format. Pick one language that interests you, and one treebank for that language, and try to builda POS tagger for this language. You can use the 4th column “UPOS” [https://universaldependencies.org/format.html](https://universaldependencies.org/format.html) Report on your findings. If you have extra time, try to experiment with various features and see if you can make your accuracy go up. You can check here [https://universaldependencies.org/conll18/results-upos.html](https://universaldependencies.org/conll18/results-upos.html) what the state of the art roughly is for your selected language and treebank. Did you come close?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.svm\n",
    "import sklearn.metrics\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: neg\n",
      "text: the single worst film i've ever seen in a theater. i saw this film at the austin film festival in 2004, and it blew my mind that this film was accepted to a festival. it was an interesting premise, and seemed like it could go somewhere, but just fell apart every time it tried to do anything. first of all, if you're going to do a musical, find someone with musical talent. the music consisted of cheesy piano playing that sounded like they were playing it on a stereo in the room they were filming. the lyrics were terribly written, and when they weren't obvious rhymes, they were groan-inducing rhymes that showed how far they were stretching to try to make this movie work. and you'd think you'd find people who could sing when making a musical, right? not in this case. luckily they were half talking/half singing in rhyme most of the time, but when they did sing it made me cringe. especially when they attempted to sing in harmony. and that just addresses the music. some of the acting was pretty good, but a lot of the dialog was terrible, as well as most of the scenes. they obviously didn't have enough coverage on the scenes, or they just had a bad editor, because they consistently jumped the line and used terrible choices while cutting the film. at least the director was willing to admit that no one wanted the script until they added the hook of making it a musical. i hope the investors make sure someone can write music before making the same mistake again.\n"
     ]
    }
   ],
   "source": [
    "with open(\"imdb_train.json\") as f:\n",
    "    data = json.load(f)\n",
    "random.seed(10) # Seed to replicate same scenario for development\n",
    "random.shuffle(data) # Shuffle data \n",
    "\n",
    "# Preview of data\n",
    "print(\"class label:\", data[0][\"class\"])\n",
    "print(\"text:\", data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate texts and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of texts: 25000\n",
      "Amount of labels 25000\n",
      "\n",
      "neg the single worst film i've ever seen in a theater....\n",
      "pos I think the reason for all the opinionated diarrhe...\n",
      "neg This movie is horrible! It rivals \\Ishtar\\\" in the...\n",
      "neg This may not be the worst comedy of all time, but ...\n",
      "pos I found this film to funny from the start. John Wa...\n",
      "pos The problem is that the movie rode in on the coatt...\n",
      "neg I was so looking forward to seeing this when it wa...\n",
      "neg I actually saw this movie in the theater back in i...\n",
      "neg blows my mind how this movie got made. i watched i...\n",
      "neg Amateurish in the extreme. Camera work especially ...\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts and labels into separate lists\n",
    "texts=[d[\"text\"] for d in data]\n",
    "labels=[d[\"class\"] for d in data]\n",
    "print(\"Amount of texts:\", len(texts))\n",
    "print(\"Amount of labels\", len(labels))\n",
    "print()\n",
    "for label, text in list(zip(labels, texts))[:10]:\n",
    "    print(label, text[:50] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test sklearn’s TfidfVectorizer\n",
    "\n",
    "**Test sklearn’s TfidfVectorizer in place of CountVectorizer on the IMDB data. Do you see any difference in the classification results or the optimal C value?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(texts, labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 100000, binary=True, ngram_range = (1,1))\n",
    "feature_matrix_train = vectorizer.fit_transform(train_texts)\n",
    "feature_matrix_dev = vectorizer.transform(dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68271)\n",
      "(5000, 68271)\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix_train.shape)\n",
    "print(feature_matrix_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.0005, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = sklearn.svm.LinearSVC(C = 0.0005, verbose = 1)\n",
    "classifier.fit(feature_matrix_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV 0.8682\n",
      "TRAIN 0.8942\n"
     ]
    }
   ],
   "source": [
    "print(\"DEV\", classifier.score(feature_matrix_dev, dev_labels))\n",
    "print(\"TRAIN\", classifier.score(feature_matrix_train, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features = 100000, binary=True, ngram_range = (1,1))\n",
    "tfidf_feature_matrix_train = tfidf_vectorizer.fit_transform(train_texts)\n",
    "tfidf_feature_matrix_dev = tfidf_vectorizer.transform(dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68271)\n",
      "(5000, 68271)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_feature_matrix_train.shape)\n",
    "print(tfidf_feature_matrix_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.0005, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_classifier = sklearn.svm.LinearSVC(C = 0.0005, verbose = 1)\n",
    "tfidf_classifier.fit(tfidf_feature_matrix_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV 0.8344\n",
      "TRAIN 0.84555\n"
     ]
    }
   ],
   "source": [
    "print(\"DEV\", tfidf_classifier.score(tfidf_feature_matrix_dev, dev_labels))\n",
    "print(\"TRAIN\", tfidf_classifier.score(tfidf_feature_matrix_train, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When comparing the CountVectorizer and TfidfVectorizer,** \n",
    "\n",
    "the first thing i noticed was that the classification results were pretty similar. The TfidfVectorizers results where lower from the start when using C value 0.0005. When  increasing the C value on both, the results of the TfidfVectorizer increased both on the dev and train. The dev value increased from 0.835 -> 0.89 and train increased from 0.846 -> 0.986. I'd say the optimal value would be around ??? where dev and train is close mebe?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test different lengths of n-grams in the CountVectorizer on the IMDB data\n",
    "\n",
    "**Test different lengths of n-grams in the CountVectorizer on the IMDB data. Do you see any difference in the classification results or the optimal C value ? Do these n-grams show up also in the list of most significant positive/negative features?**\n",
    "\n",
    "\n",
    "???limit is in 2-3 any longer becomes too unique???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 10000000, binary=True, ngram_range = (1,2))\n",
    "feature_matrix_train = vectorizer.fit_transform(train_texts)\n",
    "feature_matrix_dev = vectorizer.transform(dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = sklearn.svm.LinearSVC(C = 0.5, verbose = 1)\n",
    "classifier.fit(feature_matrix_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV 0.893\n",
      "TRAIN 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"DEV\", classifier.score(feature_matrix_dev, dev_labels))\n",
    "print(\"TRAIN\", classifier.score(feature_matrix_train, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start values:\n",
    "\n",
    "Ngram (1,1)\n",
    "\n",
    "C = 0.0005\n",
    "DEV 0.8682\n",
    "TRAIN 0.8942\n",
    "\n",
    "C = 0.005\n",
    "DEV 0.8816\n",
    "TRAIN 0.95685\n",
    "\n",
    "C = 0.05\n",
    "DEV 0.873\n",
    "TRAIN 0.9962\n",
    "\n",
    "C = 0.5\n",
    "DEV 0.8582\n",
    "TRAIN 1.0\n",
    "\n",
    "\n",
    "Ngram (1,2)\n",
    "\n",
    "C = 0.0005\n",
    "DEV 0.8854\n",
    "TRAIN 0.95205\n",
    "\n",
    "C = 0.005\n",
    "DEV 0.8978\n",
    "TRAIN 0.9989\n",
    "\n",
    "C = 0.05\n",
    "DEV 0.8944\n",
    "TRAIN 1.0\n",
    "\n",
    "C = 0.5\n",
    "DEV 0.893\n",
    "TRAIN 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('corridors', 260929), ('of', 779454), ('time', 1144551), ('the', 1098818), ('movie', 731701), ('you', 1287220), ('can', 195974), ('watch', 1228084), ('if', 548648), ('re', 904983), ('looking', 665997), ('for', 423303), ('sophisticated', 1026469), ('way', 1231077), ('suicide', 1064988), ('some', 1018977), ('use', 1198707), ('guns', 483478), ('ropes', 942565), ('or', 809802), ('gas', 450621), ('but', 182754), ('want', 1219764), ('to', 1148742), ('ruin', 945156), ('your', 1289974), ('brains', 169550), ('do', 318558), ('not', 767443), ('wait', 1217905)] ...\n"
     ]
    }
   ],
   "source": [
    "print(list(vectorizer.vocabulary_.items())[:30],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse the dictionary\n",
    "index2feature={}\n",
    "for feature,idx in vectorizer.vocabulary_.items():\n",
    "    assert idx not in index2feature #This really should hold\n",
    "    index2feature[idx]=feature\n",
    "#Now we can query index2feature to get the feature names as we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1277371  113994  164164 ... 1066919  843665  375235]\n",
      "worst\n",
      "awful\n",
      "boring\n",
      "terrible\n",
      "waste\n",
      "poor\n",
      "dull\n",
      "poorly\n",
      "the worst\n",
      "bad\n",
      "disappointment\n",
      "horrible\n",
      "disappointing\n",
      "not worth\n",
      "weak\n",
      "mess\n",
      "than this\n",
      "oh\n",
      "laughable\n",
      "save\n",
      "unfortunately\n",
      "lacks\n",
      "badly\n",
      "worse\n",
      "lame\n",
      "avoid\n",
      "not good\n",
      "not even\n",
      "nothing\n",
      "ridiculous\n",
      "-------------------------------\n",
      "excellent\n",
      "perfect\n",
      "superb\n",
      "great\n",
      "enjoyable\n",
      "amazing\n",
      "wonderful\n",
      "well worth\n",
      "gem\n",
      "better than\n",
      "today\n",
      "incredible\n",
      "rare\n",
      "definitely worth\n",
      "brilliant\n",
      "must see\n",
      "fantastic\n",
      "enjoyed\n",
      "job\n",
      "masterpiece\n",
      "fun\n",
      "the best\n",
      "refreshing\n",
      "10 10\n",
      "fascinating\n",
      "enjoyed it\n",
      "so well\n",
      "to all\n",
      "tears\n",
      "not bad\n"
     ]
    }
   ],
   "source": [
    "indices=numpy.argsort(classifier.coef_[0])\n",
    "print(indices)\n",
    "for idx in indices[:30]:\n",
    "    print(index2feature[idx])\n",
    "print(\"-------------------------------\")\n",
    "for idx in indices[::-1][:30]: #you can also do it the other way round, reverse, then pick\n",
    "    print(index2feature[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the ngram features showed up for the ngram (1,2)   ( 10 of 30 of positive) (5 of 30 negatives)\n",
    "\n",
    "range (1,3) no 3 amount showed up. for 3 words to appear consecutively is very low. \n",
    "\n",
    "range (2,3) had one \"of the worst\" in negative. No in positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worst\n",
    "waste\n",
    "poorly\n",
    "lousy\n",
    "laughable\n",
    "awful\n",
    "disappointment\n",
    "refer\n",
    "etta\n",
    "boring\n",
    "unfunny\n",
    "terrible\n",
    "disappointing\n",
    "lacks\n",
    "avoid\n",
    "dreadful\n",
    "mess\n",
    "wooden\n",
    "stupidity\n",
    "programming\n",
    "save\n",
    "miscast\n",
    "dysfunctional\n",
    "tunnel\n",
    "guilty\n",
    "outer\n",
    "fails\n",
    "skip\n",
    "hearts\n",
    "extremelly\n",
    "-------------------------------\n",
    "scariest\n",
    "refreshing\n",
    "unexpecting\n",
    "unpretentious\n",
    "hooked\n",
    "carrey\n",
    "waited\n",
    "relax\n",
    "superb\n",
    "delightful\n",
    "excellent\n",
    "perfect\n",
    "jolie\n",
    "units\n",
    "enjoyable\n",
    "fez\n",
    "steals\n",
    "eliminate\n",
    "tears\n",
    "freedom\n",
    "goof\n",
    "definitive\n",
    "sublime\n",
    "slide\n",
    "underrated\n",
    "thingy\n",
    "judged\n",
    "shines\n",
    "angelina\n",
    "dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train an SVM classifier for language recognition\n",
    "\n",
    "In the data package for the course [http://dl.turkunlp.org/intro-to-nlp.tar.gz](http://dl.turkunlp.org/intro-to-nlp.tar.gz), the directory language_identification contains data for 5 languages. Based on this data, train an SVM classifier for language recognition between these 5 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openTextFile(path):\n",
    "    with open(path) as f:\n",
    "        languageList  = []\n",
    "        for line in f:\n",
    "            languageList.append(line)\n",
    "\n",
    "    random.seed(10) # Seed to replicate same scenario for development\n",
    "    random.shuffle(languageList) # Shuffle data \n",
    "\n",
    "    return languageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_text_train =[]\n",
    "lang_label_train =[]\n",
    "\n",
    "languagePathsTrain = [\"./language_identification/en_train.txt\", \n",
    "                \"./language_identification/es_train.txt\",\n",
    "                \"./language_identification/et_train.txt\",\n",
    "                 \"./language_identification/fi_train.txt\",\n",
    "                 \"./language_identification/pt_train.txt\"]\n",
    "\n",
    "languageLabels = ['en', 'es', 'et', 'fi', 'pt']\n",
    "\n",
    "iterator = 0\n",
    "for path in languagePathsTrain:\n",
    "    lang = openTextFile(path)\n",
    "    lang_text_train.extend(lang)\n",
    "    label = [languageLabels[iterator]] * len(lang)\n",
    "    lang_label_train.extend(label)\n",
    "    iterator += 1\n",
    "\n",
    "# Read in dev txt\n",
    "lang_text_dev =[]\n",
    "lang_label_dev =[]\n",
    "    \n",
    "languagePathsDev = [\"./language_identification/en_devel.txt\", \n",
    "                \"./language_identification/es_devel.txt\",\n",
    "                \"./language_identification/et_devel.txt\",\n",
    "                 \"./language_identification/fi_devel.txt\",\n",
    "                 \"./language_identification/pt_devel.txt\"]    \n",
    "\n",
    "iterator = 0\n",
    "for path in languagePathsDev:\n",
    "    lang = openTextFile(path)\n",
    "    lang_text_dev.extend(lang)\n",
    "    label = [languageLabels[iterator]] * len(lang)\n",
    "    lang_label_dev.extend(label)\n",
    "    iterator += 1\n",
    "\n",
    "# Read in test txt\n",
    "lang_text_test =[]\n",
    "lang_label_test =[]\n",
    " \n",
    "languagePathsTest = [\"./language_identification/en_test.txt\", \n",
    "                \"./language_identification/es_test.txt\",\n",
    "                \"./language_identification/et_test.txt\",\n",
    "                 \"./language_identification/fi_test.txt\",\n",
    "                 \"./language_identification/pt_test.txt\"]    \n",
    "    \n",
    "iterator = 0\n",
    "for path in languagePathsTest:\n",
    "    lang = openTextFile(path)\n",
    "    lang_text_test.extend(lang)\n",
    "    label = [languageLabels[iterator]] * len(lang)\n",
    "    lang_label_test.extend(label)\n",
    "    iterator += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature matirx and svm train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_vectorizer = TfidfVectorizer(max_features = 100000, binary=True, ngram_range = (1,1))\n",
    "lang_feature_matrix_train = lang_vectorizer.fit_transform(lang_text_train)\n",
    "lang_feature_matrix_dev = lang_vectorizer.transform(lang_text_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28620)\n",
      "(5000, 28620)\n"
     ]
    }
   ],
   "source": [
    "print(lang_feature_matrix_train.shape)\n",
    "print(lang_feature_matrix_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_classifier = sklearn.svm.LinearSVC(C = 0.5, verbose = 1)\n",
    "lang_classifier.fit(lang_feature_matrix_train, lang_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV 0.9448\n",
      "TRAIN 0.9996\n"
     ]
    }
   ],
   "source": [
    "print(\"DEV\", lang_classifier.score(lang_feature_matrix_dev, lang_label_dev))\n",
    "print(\"TRAIN\", lang_classifier.score(lang_feature_matrix_train, lang_label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
